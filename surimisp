#!/usr/bin/python
"""
Copyright(C) 2014, Stamus Networks
Written by Eric Leblond <eleblond@stamus-networks.com>

"""

import argparse
import logging
import time
import simplejson as json
from pygtail import Pygtail
from subprocess import call
import requests
import os
import sys
import yaml
from elasticsearch import Elasticsearch
import redis
from datetime import datetime, timedelta

from threading import *
from Queue import *

MISP_URLS = { 'hostname': 'attributes/text/download/hostname', 'domain': 'attributes/text/download/domain',
         'url': 'attributes/text/download/url' }

q = Queue()

have_daemon = True
try:
    import daemon
except:
    logging.warning("No daemon support available, install python-daemon if feature is needed")
    have_daemon = False

config = {}
try:
    with open('/etc/surimisp/surimisp.conf', 'r') as conffile:
        config = yaml.load(conffile)
except:
    pass

def get_from_conf(config, key, def_value):
    if config.has_key(key):
        return config[key]
    else:
        return def_value

PROXY_PARAMS = get_from_conf(config, 'proxy_params', None)
WHITELIST = get_from_conf(config, 'whitelist', [])

parser = argparse.ArgumentParser(description='Suricata MISP IOC script')
parser.add_argument('-f', '--file', default=get_from_conf(config, 'file', '/var/log/suricata/eve.json'), help='JSON file to monitor')
parser.add_argument('-a', '--alerts', default=get_from_conf(config, 'alerts', '/var/log/suricata/ioc.json'), help='JSON file to store events to')
parser.add_argument('-v', '--verbose', default=get_from_conf(config, 'verbose', False), action="count", help="Show verbose output, use multiple times increase verbosity")
parser.add_argument('-l', '--log', default=get_from_conf(config, 'log', None), help='File to log output to (default to stdout)')
parser.add_argument('-o', '--offset', default=get_from_conf(config, 'offset', None), help='File to write offset to')
parser.add_argument('-b', '--batch', default=get_from_conf(config, 'batch', False), action="store_true", help="Read file and exit at end")
parser.add_argument('-w', '--workers', default=get_from_conf(config, 'workers', 1), type=int, help='Number of alert workers to start')
parser.add_argument('-u', '--url', default=get_from_conf(config, 'url', None), help='Set option to url where JSON file to monitor, if unset no refresh')
parser.add_argument('-e', '--elasticsearch', default=get_from_conf(config, 'elasticsearch', None), help='Set elasticsearch server and use it as input, if unset use file')
parser.add_argument('-r', '--redis', default=get_from_conf(config, 'redis', None), help='Set redis server and use it as input, if unset use file')
parser.add_argument('-i', '--interval', default=get_from_conf(config, 'interval', 3600), type=int, help='Interval between file update in second')
parser.add_argument('-d', '--basedir', default=get_from_conf(config, 'basedir', '/var/lib/surimisp/'), help='Directory where data will stay')
parser.add_argument('-k', '--apikey', default=get_from_conf(config, 'apikey', None), help='API key to use')
parser.add_argument('-S', '--strict', default=get_from_conf(config, 'strict', False), action="store_true", help='Be strict on TLS checks')

ALERT_SUBOBJECT = { "hostname": { "action": "allowed", "category": "Misc Attack", "gid": 1, "rev": 4, "severity": 3, "signature": "IOC alert on HTTP hostname", "signature_id": 1 },
"domain": { "action": "allowed", "category": "Misc Attack", "gid": 1, "rev": 4, "severity": 3, "signature": "IOC alert on DNS request name", "signature_id": 2 },
"url": { "action": "allowed", "category": "Misc Attack", "gid": 1, "rev": 4, "severity": 3, "signature": "IOC alert on HTTP url", "signature_id": 3 },
"ip": { "action": "allowed", "category": "Misc Attack", "gid": 1, "rev": 4, "severity": 3, "signature": "IOC alert on IP address", "signature_id": 4 } }

if have_daemon:
    parser.add_argument('-D', '--daemon', default=False, action="store_true", help="Run as unix daemon")

args = parser.parse_args()

if args.url and not args.apikey:
    print("URL specified and no API key aborting.")
    sys.exit(1)

if args.verbose >= 3:
    loglevel=logging.DEBUG
elif args.verbose >= 2:
    loglevel=logging.INFO
elif args.verbose >= 1:
    loglevel=logging.WARNING
else:
    loglevel=logging.ERROR

hostname_list = None
domain_list = None
url_list = None
count = { 'hostname': 0, 'url': 0, 'domain': 0 }

def setup_logging(args):
    if args.log:
        logging.basicConfig(filename=args.log,
                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                            level=loglevel)
    else:
        logging.basicConfig(level=loglevel)

def fetch_data(baseurl, basedir):
    session = requests.Session()
    session.headers.update(
            {'Authorization': args.apikey }
            )
    for url in MISP_URLS:
        resp = session.get(baseurl + MISP_URLS[url], proxies = PROXY_PARAMS, verify = args.strict)
        fpath = os.path.join(basedir, url)
        fdata = open(fpath, 'w')
        fdata.write(resp.content)
        fdata.close()

def load_data(dfile):
    iocfile = open(dfile, 'r')
    entries = []
    for line in iocfile:
        entry = line.rstrip('\n')
        skip = False
        for domain in WHITELIST:
            if entry.endswith(domain):
                skip = True
                break
        if not skip:
            entries.append(entry)
    return set(entries)

def load_all_data(basedir):
    global hostname_list
    global domain_list
    global url_list
    hostname_list = load_data(os.path.join(basedir, 'hostname'))
    domain_list = load_data(os.path.join(basedir, 'domain'))
    url_list = load_data(os.path.join(basedir, 'url'))
 
def check_http(event):
    global count
    try:
        if event['http']['hostname'] in hostname_list:
           event['ioc'] = 'hostname'
           event['alert'] = ALERT_SUBOBJECT['hostname']
           q.put(event)
           count['hostname'] = count['hostname'] + 1
        if event['http']['url'] in url_list:
           event['ioc'] = 'url'
           event['alert'] = ALERT_SUBOBJECT['url']
           q.put(event)
           count['url'] = count['url'] + 1
    except:
        pass

def check_dns(event):
    try:
        if event['dns']['rrname'] in domain_list:
           event['ioc'] = 'domain'
           event['alert'] = ALERT_SUBOBJECT['domain']
           q.put(event)
           count['domain'] = count['domain'] + 1
    except:
        pass

def AlertSender(mode = 'file', alerts = None):
    while True:
        event = q.get()
        # Switch event to alert
        event['event_type'] = 'alert'
        if mode == 'file':
            alerts.write(json.dumps(event) + '\n')
            alerts.flush()
        q.task_done()


def FetchData(interval = 3600, url = None, basedir = None):
    while 1:
        time.sleep(float(interval))
        logging.info("Updating IOC lists")
        fetch_data(url, basedir)
        load_all_data(basedir)

def treat_json_file(args):
    if args.batch:
        source = open(args.file, 'r')
    else:
        source = Pygtail(args.file, offset_file=args.offset)

    if args.batch:
        start_time = time.clock()
    while 1:
        for line in source:
            try:
                event = json.loads(line)
            except json.decoder.JSONDecodeError:
                continue
            if event.has_key('event_type'):
                if event['event_type'] == 'http':
                    check_http(event)
                elif event['event_type'] == 'dns':
                    check_dns(event)
        if args.batch:
            break
        else:
            time.sleep(0.1)
            source = Pygtail(args.file, offset_file=args.offset)

    if args.batch:
        end_time = time.clock()
        logging.info("Matching took %fs" % (end_time - start_time))
        logging.info("Count: " + repr(count))

def treat_redis_publisher(args):
    r = redis.StrictRedis(host=args.redis)
    p = r.pubsub()
    # FIXME hardcoded ....
    p.psubscribe('logstash-http', 'logstash-dns')
    while 1:
        msg = p.get_message()
        if msg:
            if msg['type'] == 'pmessage':
                event = json.loads(msg['data'])
                if event.has_key('event_type'):
                    if event['event_type'] == 'http':
                        check_http(event)
                    elif event['event_type'] == 'dns':
                        check_dns(event)
        else:
            time.sleep(0.1)


def treat_redis(args):
    r = redis.StrictRedis(host=args.redis)
    while 1:
        # FIXME hardcoded ....
        msg = r.rpoplpush('logstash-events', 'logstash')
        if msg:
            event = json.loads(msg)
            if event.has_key('event_type'):
                if event['event_type'] == 'http':
                    check_http(event)
                elif event['event_type'] == 'dns':
                    check_dns(event)
        else:
            time.sleep(0.1)

def treat_elasticsearch(args):
    es = Elasticsearch([args.elasticsearch])
    # FIXME real date
    orig_timestamp_str = '2014-03-08T13:43:22.551756'
    orig_timestamp = datetime.strptime(orig_timestamp_str,'%Y-%m-%dT%H:%M:%S.%f')
    end_timestamp = orig_timestamp + timedelta(hours = 1)

    query = '(event_type:http OR event_type:dns) AND timestamp:["%s" TO "%s"}' % (orig_timestamp.strftime('%Y-%m-%dT%H:%M:%S.%f'), end_timestamp.strftime('%Y-%m-%dT%H:%M:%S.%f'))
    results = es.search(q = query, index='_all', ignore_unavailable = True, scroll = 30, size = 20, search_type = 'scan')

    scroll_id = results['_scroll_id']

    while 1:
        for entry in results['hits']['hits']:
            event = entry['_source']
            if event.has_key('event_type'):
                if event['event_type'] == 'http':
                    check_http(event)
                elif event['event_type'] == 'dns':
                    check_dns(event)
        print "Scrolling once more"
        results = es.scroll(scroll_id = scroll_id, scroll = 10)
        print results['hits']['hits'][0]

def main_task(args):
    setup_logging(args)

    if args.url:
        fetch_data(args.url,args.basedir)
        if args.interval:
            t = Thread(target=FetchData, kwargs = {'interval': args.interval, 'url': args.url, 'basedir': args.basedir })
            t.daemon = True
            t.start()

    load_all_data(args.basedir)

    alerts = open(args.alerts, 'a+')

    q = Queue()
    for i in range(args.workers):
        t = Thread(target=AlertSender, kwargs = {'mode': 'file', 'alerts': alerts })
        t.daemon = True
        t.start()

    if args.batch:
        start_time = time.clock()

    if args.batch:
        end_time = time.clock()
        logging.info("Building sets took %fs" % (end_time - start_time))

    if args.redis:
        treat_redis(args)
    elif args.elasticsearch:
        treat_elasticsearch(args)
    else:
        treat_json_file(args)

if have_daemon and args.daemon:
    with daemon.DaemonContext():
        main_task(args)
else:
    main_task(args)
