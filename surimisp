#!/usr/bin/python
"""
Copyright(C) 2014, Stamus Networks
Written by Eric Leblond <eleblond@stamus-networks.com>

"""

import argparse
import logging
import time
import simplejson as json
from pygtail import Pygtail
from subprocess import call
import requests
import os
import sys
import yaml

from threading import *
from Queue import *

MISP_URLS = { 'hostname': 'attributes/text/download/hostname', 'domain': 'attributes/text/download/domain',
         'url': 'attributes/text/download/url' }

EUC_WHITELIST = ['google.com', 'microsoft.com']

q = Queue()

have_daemon = True
try:
    import daemon
except:
    logging.warning("No daemon support available, install python-daemon if feature is needed")
    have_daemon = False

config = {}
try:
    with open('/etc/surimisp/surimisp.conf', 'r') as conffile:
        config = yaml.load(conffile)
except:
    pass

def get_from_conf(config, key, def_value):
    if config.has_key(key):
        return config[key]
    else:
        return def_value

PROXY_PARAMS = get_from_conf(config, 'proxy_params', None)

parser = argparse.ArgumentParser(description='Suricata MISP IOC script')
parser.add_argument('-f', '--file', default=get_from_conf(config, 'file', '/var/log/suricata/eve.json'), help='JSON file to monitor')
parser.add_argument('-a', '--alerts', default=get_from_conf(config, 'file', '/var/log/suricata/ioc.json'), help='JSON file to store events to')
parser.add_argument('-v', '--verbose', default=get_from_conf(config, 'verbose', False), action="count", help="Show verbose output, use multiple times increase verbosity")
parser.add_argument('-l', '--log', default=get_from_conf(config, 'log', None), help='File to log output to (default to stdout)')
parser.add_argument('-o', '--offset', default=get_from_conf(config, 'offset', None), help='File to write offset to')
parser.add_argument('-b', '--batch', default=get_from_conf(config, 'batch', False), action="store_true", help="Read file and exit at end")
parser.add_argument('-w', '--workers', default=get_from_conf(config, 'workers', 1), type=int, help='Number of alert workers to start')
parser.add_argument('-u', '--url', default=get_from_conf(config, 'url', None), help='Set option to url where JSON file to monitor, if unset no refresh')
parser.add_argument('-i', '--interval', default=get_from_conf(config, 'interval', 3600), type=int, help='Interval between file update in second')
parser.add_argument('-d', '--basedir', default=get_from_conf(config, 'basedir', '/var/lib/surimisp/'), help='Directory where data will stay')
parser.add_argument('-k', '--apikey', default=get_from_conf(config, 'apikey', None), help='API key to use')

if have_daemon:
    parser.add_argument('-D', '--daemon', default=False, action="store_true", help="Run as unix daemon")

args = parser.parse_args()

if args.url and not args.apikey:
    print("URL specified and no API key aborting.")
    sys.exit(1)

if args.verbose >= 3:
    loglevel=logging.DEBUG
elif args.verbose >= 2:
    loglevel=logging.INFO
elif args.verbose >= 1:
    loglevel=logging.WARNING
else:
    loglevel=logging.ERROR

hostname_list = None
domain_list = None
url_list = None
count = { 'hostname': 0, 'url': 0, 'domain': 0 }

def setup_logging(args):
    if args.log:
        logging.basicConfig(filename=args.log,
                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                            level=loglevel)
    else:
        logging.basicConfig(level=loglevel)

def fetch_data(baseurl, basedir):
    session = requests.Session()
    session.headers.update(
            {'Authorization': args.apikey }
            )
    for url in MISP_URLS:
        resp = session.get(baseurl + MISP_URLS[url], proxies = PROXY_PARAMS)
        fpath = os.path.join(basedir, url)
        fdata = open(fpath, 'w')
        fdata.write(resp.content)
        fdata.close()

def load_data(dfile):
    iocfile = open(dfile, 'r')
    entries = []
    for line in iocfile:
        entry = line.rstrip('\n')
        skip = False
        for domain in EUC_WHITELIST:
            if entry.endswith(domain):
                skip = True
                break
        if not skip:
            entries.append(entry)
    return set(entries)

def load_all_data(basedir):
    global hostname_list
    global domain_list
    global url_list
    hostname_list = load_data(os.path.join(basedir, 'hostname'))
    domain_list = load_data(os.path.join(basedir, 'domain'))
    url_list = load_data(os.path.join(basedir, 'url'))
 
def check_http(event):
    global count
    try:
        if event['http']['hostname'] in hostname_list:
           event['ioc'] = 'hostname'
           q.put(event)
           count['hostname'] = count['hostname'] + 1
        if event['http']['url'] in url_list:
           event['ioc'] = 'url'
           q.put(event)
           count['url'] = count['url'] + 1
    except:
        pass

def check_dns(event):
    try:
        if event['dns']['rrname'] in domain_list:
           event['ioc'] = 'domain'
           q.put(event)
           count['domain'] = count['domain'] + 1
    except:
        pass

def AlertSender(mode = 'file', alerts = None):
    while True:
        event = q.get()
        # Switch event to alert
        event['event_type'] = 'alert'
        if mode == 'file':
            alerts.write(json.dumps(event) + '\n')
        q.task_done()


def FetchData(interval = 3600, url = None, basedir = None):
    while 1:
        time.sleep(float(interval))
        logging.info("Updating IOC lists")
        fetch_data(url, basedir)
        load_all_data(basedir)

def main_task(args):
    setup_logging(args)

    if args.url:
        fetch_data(args.url,args.basedir)
        if args.interval:
            t = Thread(target=FetchData, kwargs = {'interval': args.interval, 'url': args.url, 'basedir': args.basedir })
            t.daemon = True
            t.start()

    load_all_data(args.basedir)

    alerts = open(args.alerts, 'a+')

    q = Queue()
    for i in range(args.workers):
        t = Thread(target=AlertSender, kwargs = {'mode': 'file', 'alerts': alerts })
        t.daemon = True
        t.start()

    if args.batch:
        start_time = time.clock()

    if args.batch:
        end_time = time.clock()
        logging.info("Building sets took %fs" % (end_time - start_time))

    # init MISP data
    #fetch_misp_data() 
    if args.batch:
        source = open(args.file, 'r')
    else:
        source = Pygtail(args.file, offset_file=args.offset)

    if args.batch:
        start_time = time.clock()
    while 1:
        for line in source:
            try:
                event = json.loads(line)
            except json.decoder.JSONDecodeError:
                continue
            if event.has_key('event_type'):
                if event['event_type'] == 'http':
                    check_http(event)
                elif event['event_type'] == 'dns':
                    check_dns(event)
        if args.batch:
            break
        else:
            time.sleep(0.1)
            source = Pygtail(args.file, offset_file=args.offset)

    if args.batch:
        end_time = time.clock()
        logging.info("Matching took %fs" % (end_time - start_time))
        logging.info("Count: " + repr(count))

if have_daemon and args.daemon:
    with daemon.DaemonContext():
        main_task(args)
else:
    main_task(args)
